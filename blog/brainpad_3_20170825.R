# ***********************************************************************************************
# Title     : 1-3. レコメンド精度の評価方法を学ぶ
# Objective : TODO
# Created by: Owner
# Created on: 2021/02/01
# URL       : https://blog.brainpad.co.jp/entry/2017/08/25/140000
# ***********************************************************************************************


# ＜レコメンドの評価とは＞
# - 与えられたケースにおける良いレコメンドとは何かを決定する枠組み
#   --- 状況や目的に応じた様々な評価方法や指標があり、指標も日々新たに考案されアップデートされている
#   --- 最終的にレコメンドに求めるビジネス面のゴールやKPIによって決定される


# ＜評価方法＞
# - オフライン評価： 一般的な機械学習の評価手法
# - オンライン評価： 実サービスにレコメンドを実装してモデルを評価（A/Bテスト）


# ＜オフライン評価＞
# ［手順］
# - 1. 利用履歴を学習データとテストデータに分割
# - 2. 学習データのみでレコメンドモデルを構築
# - 3. 構築したモデルをテストデータに適用して、ユーザのレーティングや購買を予測し
# - 4. どの程度正確に言い当てられるかを評価

# ［課題］
# - アウトオブサンプル評価ではユーザーがモデルの推奨にかかわらず購入する
#   --- それはモデルのレコメンドと大きく異なり、同列の評価が適切かは議論の余地が残る
#   --- ｢実サービスの評価｣はどの分野でも難しい

# ［評価指標］
#   ●数値データ評価
# - 1.予測精度（MAE、MSE、RMSE、Logloss）
#   ●バイナリデータ評価
# - 2.分類精度（Precision、Recall、F値）
# - 3.ランキング精度（ROC曲線、PR曲線AUC、MRR、MAPE、nDCG）
# - 4.カバレッジ（カタログカバレッジ、ユーザーカバレッジ）
# - 5.その他（目新しさ、セレンディピティ）


# ＜オンライン評価＞
# ［手順］
# - 1.対象ユーザをランダムに振り分ける
# - 2 以下の2群に分けて結果を比較して評価（ABテスト）
#     --- 一方のユーザには既存のロジックのレコメンドを提示（コントロール）
#     --- もう一方は比較したい新しいロジックを提示して（トリートメント）

# ［課題］
# - オンゴーイングでテストするので機会損失につながりうる
# - 手間やリスクも大きくオフラインほど手軽ではない
# - オフライン評価で良い精度のアルゴリズムが、必ずしも実サービスでいい精度を出せるとは限らない


# ［評価指標］
# - クリック率（CTR：Click-Through Rate）
# - コンバージョン率（CVR：Conversion Rate）

